---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.16.0
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
  orphan: true
---

# Fitting models with different cost functions

We also get on to cross-validation.

```{python}
import numpy as np
np.set_printoptions(suppress=True)
from scipy.optimize import minimize
import pandas as pd
pd.set_option('mode.copy_on_write', True)
import statsmodels.formula.api as smf
import sklearn.linear_model as sklm
import sklearn.metrics as skmetrics
```

```{python}
df = pd.read_csv('data/rate_my_course.csv')
df
```

Fetch some columns of interest:

```{python}
# This will be our y (the variable we predict).
helpfulness = df['Helpfulness']
# One of both of these will be our X (the predictors).
clarity_easiness = df[['Clarity', 'Easiness']]
```

Fit the model with Statsmodels.

```{python}
sm_model = smf.ols('Helpfulness ~ Clarity + Easiness', data=df)
sm_fit = sm_model.fit()
sm_fit.summary()
```

Fit the same model with Scikit-learn.

```{python}
sk_model = sklm.LinearRegression()
sk_fit = sk_model.fit(clarity_easiness, helpfulness)
sk_fit
```

The coefficients (the slopes for the regressors):

```{python}
sk_fit.coef_
```

The intercept:

```{python}
sk_fit.intercept_
```

Compare the parameters to Statsmodels:


```{python}
sm_fit.params
```

## The fitted values and Scikit-learn

The values predicted by the (Sklearn) model:

```{python}
y_hat = sk_fit.predict(clarity_easiness)
y_hat
```

Compare the fitted ($\hat{y}$) values to those from Statsmodels:

```{python}
sm_fit.predict() - y_hat
```

```{python}
assert np.allclose(sm_fit.predict(), y_hat)
```

We assemble Sklearn's coefficients and intercept into a single list,
with the intercept last.

```{python}
params = list(sk_fit.coef_) + [sk_fit.intercept_]
params
```

If we just want all but the last parameter (all the coefficients, but not the intercept:

```{python}
# All parameters but the last (all but the intercept parameter).
params[:-1]
```

We could also get the parameters from Statsmodels, but we'd have to rearrange them, because Statsmodels puts the intercept first rather than last:

```{python}
sm_fit.params.iloc[[1, 2, 0]]
```

Write a function to compute the fitted values given the parameters:


```{python}
def calc_fitted(params, X):
    """ Calculate fitted values from design X and parameters

    Parameters
    ----------
    params : vector (1D array)
        Vector of parameters, intercept is last parameter.
    X : array
        2D array with regressor columns.

    Returns
    -------
    y_hat : vector
        Vector of fitted values
    """
    X = np.array(X)
    n, p = X.shape
    y_hat = np.zeros(n)
    for col_no, param in enumerate(params[:-1]):
        y_hat = y_hat + param * X[:, col_no]  # Add contribution from this regressor.
    y_hat = y_hat + params[-1]  # Add contribution from intercept.
    return y_hat
```

Show that we get the same fitted values from our function as we got from
the `predict` method of Sklearn:

```{python}
our_y_hat = calc_fitted(params, clarity_easiness)
assert np.allclose(our_y_hat, y_hat)
```
```{python}
assert np.allclose(calc_fitted(params, clarity_easiness), y_hat)
```

Calculate the error vector and then calculate the sum of squared error:

```{python}
e = helpfulness - calc_fitted(params, clarity_easiness)
np.sum(e ** 2)
```

Make a function to calculate sum of squared error:

```{python}
def sos(params, X, y):
    """ Sum of squared error for `params` given model `X` and data `y`.
    """
    y_hat = calc_fitted(params, X)
    e = y - y_hat  # residuals
    return np.sum(e ** 2)
```

Check that we get the same answer from the function as we got from
calculating above:

```{python}
sos(params, clarity_easiness, helpfulness)
```

Use `minimize` to find parameters minimizing the sum of squared error:

```{python}
min_res = minimize(sos, [0, 0, 0], args=(clarity_easiness, helpfulness))
min_res
```

Yes, the parameters (coefficients and intercept) are the same as we got
from Statsmodels and Sklearn:

```{python}
min_res.x
```


## The long and the short of $R^2$

$R^2$ can also be called the [coefficient of deterimination](https://en.wikipedia.org/wiki/Coefficient_of_determination) Sklearn has an `r2_score` metric.

Sklearn has an `r2_score` metric.

```{python}
skmetrics.r2_score(helpfulness, y_hat)
```

We already know the formula for $R^2$.   If $f_i$ is the *fitted* value for observation $i$, then the residual error $e_i$ for observation $i$ is $y_i - f_i$, and the residual sum of squares is the sum of squares of the residuals: 

$$
SS_\text{res}=\sum_i (y_i - f_i)^2=\sum_i e_i^2\
$$

Write the mean of the observed values $y$ as $\bar{y}$.   Then the total
sum of squares is:

$$
SS_\text{tot}=\sum_i (y_i - \bar{y})^2
$$

and

$$
R^2 = 1 - {SS_{\rm res}\over SS_{\rm tot}}
$$

We can calculate $R^2$ by hand to show this gives the same answer as Sklearn and Statsmodels.

For notational convenient, give our `y` vector the variable name `y`:

```{python}
y = helpfulness
```

```{python}
ss_total = np.sum((y - np.mean(y)) ** 2)
ss_total
```

Here is the sum of squares from the full model:

```{python}
ss_residuals = np.sum((y - y_hat) ** 2)
ss_residuals
```

Our by-hand calculation gives the same answer as Sklearn or Statsmodels:

```{python}
# Calculate R2
1 - ss_residuals / ss_total
```

### $R^2$ for another, reduced model

Fit a reduced model that is just "Clarity" without "Easiness".

```{python}
clarity_df = df[['Clarity']]
reduced_fit = sklm.LinearRegression().fit(clarity_df, helpfulness)
reduced_fit.coef_, reduced_fit.intercept_
```

Calculate $R^2$ for reduced model as compared to our full model.

```{python}
reduced_resid = helpfulness - reduced_fit.predict(clarity_df)
ss_reduced_residuals = (reduced_resid ** 2).sum()
ss_reduced_residuals
```

```{python}
1 - ss_residuals / ss_reduced_residuals
```

## On weights, and a weighted mean

This is the usual mean, on `y` (the 'Helpfulness" scores):

```{python}
np.mean(y)
```

The calculation is:

```{python}
n = len(y)
np.sum(y) / n
```

Of course this is the same as:

```{python}
np.sum(y) * 1 / n
```

In mathematical notation, we write this as:

$$
\bar{y} = \frac{1}{n} (y_1 + y_2 + ... + y_n) \\
$$

where $\bar{y}$ is the mean of the values in the vector $\vec{y}$.

We could also write adding up the $y$ values with the $\sum$ notation, as in:

$$
\bar{y} = \frac{1}{n} \sum_i y_i
$$

```{python}
1 / n * np.sum(y)
```

Mathematically, because $p * (q + r + s) = p * q + p * r + p * s$, we
can also do the multiplication by $\frac{1}{n}$ *inside the brackets*,
like this:

$$
\bar{y} = \frac{1}{n} y_1 +
          \frac{1}{n} y_2 +
          ...
          \frac{1}{n} y_n + 
$$

With the $\sum$ notation, that would be:

$$
\bar{y} = \sum_i \frac{1}{n} y_i
$$

In code:

```{python}
np.sum(1 / n * y)
```

Think of this - the standard calculation of the mean - as giving each
value in $y$ the same *weight* - of $\frac{1}{n}$:

```{python}
weights = np.ones(n) / n
weights
```

```{python}
np.sum(y * weights)
```

Calculate weights to weight values by number of professors, on the basis that larger number of professors may give more reliable values.

```{python}
n_professors = df['Number of Professors']
n_professors
```

```{python}
total_n_professors = np.sum(n_professors)
total_n_professors
```

```{python}
prop_professors_by_subject = n_professors / total_n_professors
prop_professors_by_subject
```

```{python}
np.sum(prop_professors_by_subject)
```

Calculate weighted mean:

```{python}
# Weighted mean of helpfulness (y), weighted by number of professors.
np.sum(y * prop_professors_by_subject)
```

Numpy's version of same:

```{python}
np.average(y, weights=prop_professors_by_subject)
```

And in fact, Numpy will check whether the weights sum to 1, and if not, will automatically divide the weights by their sum, so we can get the same calculation with the raw numbers of professors:

```{python}
np.average(y, weights=n_professors)
```

## Fitting the model with minimize

A function for Sum of Squares, to use with `minimize`.

```{python}
def sos(params, X, y):
    residuals = y - calc_fitted(params, X)
    return np.sum(residuals ** 2)
```

Use `sos` function, in an example call, using Sklearn's best-fit
`params`:

```{python}
# Rename the variables for clarity.
X = clarity_easiness  # Design DataFrame
y = helpfulness  # Target variable.
sos([0, 0, 0], X, y)
```

Use `sos` with `minimize`:

```{python}
res = minimize(sos, [0, 0, 0], args=(X, y))
res
```

Notice we get the same parameters (coefficients then intercept) as we do
from Sklearn or Statsmodels:

```{python}
res.x
```

Compare to parameters with (e.g.) Sklearn.

```{python}
sk_fit.coef_, sk_fit.intercept_
```

## Fitting with weights

Statsmodels, weighted regression.

```{python}
sm_weighted_model = smf.wls('Helpfulness ~ Clarity + Easiness',
                            weights=prop_professors_by_subject,
                            data=df)
sm_weighted_fit = sm_weighted_model.fit()
sm_weighted_fit.summary()
```

[Sklearn, weighted
regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).
Also see [Wikipedia on weighted
regression](https://en.wikipedia.org/wiki/Weighted_least_squares).

```{python}
sk_weighted = sklm.LinearRegression().fit(X, y, sample_weight=prop_professors_by_subject)
sk_weighted
```

```{python}
sk_weighted.coef_, sk_weighted.intercept_
```

The `minimize` cost function for weighted regression:

```{python}
def sos_weighted(params, X, y, weights):
    """ Weighted least squares cost function
    """
    y_hat = calc_fitted(params, X)
    e = y - y_hat  # residuals
    e2 = e ** 2
    return np.sum(e2 * weights)
```

```{python}
weighted_res = minimize(sos_weighted, [0, 0, 0], args=(X, y, prop_professors_by_subject))
weighted_res
```

```{python}
weighted_res.x
```

These are the same as the parameters we got from Sklearn and
Statsmodels:

```{python}
sk_weighted.coef_, sk_weighted.intercept_
```

## Penalized regression

Penalized regression is where you simultaneously minimize some cost related to the model (mis-)fit, and some cost related to the parameters of your model.

### Ridge regression

For example, in [ridge
regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html),
with try and minimize the sum of squared residuals _and_ the sum of squares
of the parameters (apart from the intercept).

```{python}
sk_ridge = sklm.Ridge(alpha=1).fit(X, y)
sk_ridge.coef_, sk_ridge.intercept_
```

Fit with the `minimize` cost function:

```{python}
def sos_ridge(params, X, y, alpha):
    ss_resid = sos(params, X, y)  # Using our sos function.
    return ss_resid + alpha * np.sum(params[:-1] ** 2)
```

Fit ridge regression with the `minimize` cost function:

```{python}
res_ridge = minimize(sos_ridge, [0, 0, 0], args=(X, y, 1.0))
res_ridge.x
```


### LASSO


See the [Scikit-learn LASSO page](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html).


As noted there, the cost function is:

$$
\frac{1}{ 2 * n } * ||y - Xw||^2_2 + alpha * ||w||_1
$$

$w$ refers to the vector of model parameters.

This part of the equation:

$$
||y - Xw||^2_2
$$

is the sum of squares of the residuals, because the residuals are $y - Xw$
(where $w$ are the parameters of the model, and $Xw$ are therefore the fitted
values), and the $||y - Xw||^2_2$ refers to the squared [L2 vector
norm](https://mathworld.wolfram.com/L2-Norm.html), which is the same as the
sum of squares.

$$
||w||_1
$$

is the [L1 vector norm](https://mathworld.wolfram.com/L1-Norm.html) of the
parameters $w$, which is the sum of the absolute values of the parameters.

Let's do that calculation, with a low `alpha` (otherwise both slopes get
forced down to zero):

```{python}
# We need LassoLars for increased accuracy.
sk_lasso = sklm.LassoLars(alpha=0.01).fit(clarity_easiness, helpfulness)

sk_lasso.coef_, sk_lasso.intercept_
```

Here is the equivalent `minimize` cost function:

```{python}
def sos_lasso(params, X, y, alpha):
    ss_resid = sos(params, X, y)
    n = len(y)
    penalty = np.sum(np.abs(params[:-1]))
    return 1 / (2 * n) * ss_resid + alpha * penalty
```

```{python}
res_lasso = minimize(sos_lasso, [0, 0, 0], args=(X, y, 0.01))
res_lasso
```

```{python}
res_lasso.x
```

## Cross-validation

Should I add the "Easiness" regressor?  In other words, is a model that has _both_ the "Clarity" and "Easiness" regressor better than a model that just has the "Clarity" regressor?

We could start by comparing the remaining (residual) sum of squares for the two models.

```{python}
# Make a single-column dataframe with Clarity
clarity_df = clarity_easiness[['Clarity']]
clarity_df
```

First we fit a linear model with the single-regressor "Clarity" model, and calculate the sum of squared residuals:

```{python}
single_fit = sklm.LinearRegression().fit(
    clarity_df, helpfulness)
single_y_hat = single_fit.predict(clarity_df)
single_ss = np.sum((helpfulness - single_y_hat) ** 2)
single_ss
```

Then we do the same for the two-regressor model, with "Clarity" and "Easiness":

```{python}
both_fit = sklm.LinearRegression().fit(
    clarity_easiness, helpfulness)
both_y_hat = both_fit.predict(clarity_easiness)
both_ss = np.sum((helpfulness - both_y_hat) ** 2)
both_ss
```

There's a small difference:

```{python}
single_ss - both_ss
```

Notice the sum of squared error (residuals) (SSE) is very slightly lower for the model with "Easiness".   But in fact, we could show that we would reduce the error by adding almost any regressor to the model.  In fact, the error can only ever go down, or stay the same, by adding another regressor.

In fact — let's try it.  We'll add a regressor of random numbers to the "Clarity" regressor, and see how it fares:

```{python}
rng = np.random.default_rng()

with_random = clarity_df.copy()
with_random['random'] = rng.normal(size=n)
with_random
```

```{python}
random_fit = sklm.LinearRegression().fit(
    with_random, helpfulness)
random_y_hat = both_fit.predict(clarity_easiness)
random_ss = np.sum((helpfulness - random_y_hat) ** 2)
random_ss
```

```{python}
single_ss - random_ss
```

We would like a better test — one that tells us whether our model is
better able to predict a _new_ value that the model hasn't seen before.

We can do this with *cross-validation*.  *Leave one out* is a simple
form of cross-validation.  The procedure is, that we take each row in
the dataset in turn.  We drop that row from the dataset, and fit the
model (estimate the parameters) on that dataset, with the row dropped.
Then we use that model to *predict* the value from the row that we
dropped — because this a value we did not use to build the model.  We do this for all rows, making a model and predicting the value from the row we dropped.  Then we ask — how well do the predicted values do in predicting the actual values?

This how that would work for the first row:

* Drop the first row and keep it somewhere.
* Run the model on the remaining rows.
* Use model to predict target value for first row.
* Store prediction.

In code that might look like this:

```{python}
row0_label = 0  # Row label of the first row.
# Row to drop, as a data frame:
dropped_row = df.loc[row0_label:row0_label]
dropped_row
```

We'll be using these columns for the design and target:

```{python}
x_cols = ['Clarity', 'Easiness']
y_col = 'Helpfulness'
```

Fit model with remaining rows, and predict target variable for first
row:

```{python}
# Dataframe without dropped row
remaining_df = df.drop(index=row0_label)
# Fit on everything but the dropped row.
fit = sklm.LinearRegression().fit(
    remaining_df[x_cols],
    remaining_df[y_col])
# Use fit to predict the dropped row.
fitted_val = fit.predict(dropped_row[x_cols])
fitted_val
```

Then we keep going to generate a fitted value for every row.

Here's a function to do the leave-one-out fit for a given row label:


```{python}
def drop_and_predict(df, x_cols, y_col, row_label):
    """ Drop value identified by `row_label`, fit with rest and predict

    Parameters
    ----------
    df : DataFrame
    x_cols : sequence
        Sequence of column labels defining regressors.
    y_col : str
        Column label for target variable.
    row_label : object
        Row label of row to drop

    Returns
    -------
    fitted : scalar
        Fitted value for column `y_col` in row labeled `row_label`,
        using fit from all rows except row labeled `row_label`.
    """
    dropped_row = df.loc[row_label:row_label]
    # Dataframe without dropped row
    remaining_df = df.drop(index=row_label)
    # Fit on everything but the dropped row.
    fit = sklm.LinearRegression().fit(
        remaining_df[x_cols],
        remaining_df[y_col])
    # Use fit to predict target in the dropped row, and return.
    return fit.predict(dropped_row[x_cols])[0]
```

Use `drop_and_predict` to build a model for all rows but the first, as
above, and then predict the "Helpfulness" value of the first row.

```{python}
actual_value = df.loc[0, 'Helpfulness']
predicted_value = drop_and_predict(df,
                                   ['Clarity', 'Easiness'],
                                   'Helpfulness',
                                   0)
predicted_value
```

Fit the model, with both "Clarity" and "Easiness", and drop / predict
each "Helpfulness" value.

```{python}
predicted_with_easiness = df['Helpfulness'].copy()
for label in df.index:
    fitted = drop_and_predict(df,
                              ['Clarity', 'Easiness'],
                              'Helpfulness',
                              label)
    predicted_with_easiness.loc[label] = fitted
predicted_with_easiness
```

Get the sum of squared residuals for these predictions:

```{python}
error_both = df['Helpfulness'] - predicted_with_easiness
# Sum of squared prediction errors for larger model.
np.sum(error_both ** 2)
```

Do the same for the model with "Clarity" only.

```{python}
predicted_without_easiness = df['Helpfulness'].copy()
for label in df.index:
    fitted = drop_and_predict(df, ['Clarity'], ['Helpfulness'], label)
    predicted_without_easiness.loc[label] = fitted
error_just_one = df['Helpfulness'] - predicted_without_easiness
# Sum of squared prediction errors for smaller model.
np.sum(error_just_one ** 2)
```

The model without "Easiness" does a slightly *better* job of predicting, with leave-one-out cross-validation.
