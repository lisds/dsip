---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.6
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Multiple Predictor Linear Regression Models

[Page will use both Duncan data and EPL data]

[Will cover some content which is on the other pages]

Structure:

Duncan data:
- regression models with continuous predictors

Statistical adjustment:
- EPL data

Duncan data:
- regression models with categorical and continuous predictors

Model naming convention for this notebook:

`dataset_outcome_predictor1_predictor2`

`duncan_prestige_income_education`

`epl_goaldiff_forward_defense`

```{python}
# Import numerical and plotting libraries
import numpy as np
import numpy.linalg as npl
import matplotlib.pyplot as plt
import pandas as pd
from jupyprint import jupyprint, arraytex
# Only show 6 decimals when printing
np.set_printoptions(precision=6)
import statsmodels.formula.api as smf
# For interactive widgets.
from ipywidgets import interact

# cost function for two predictors (used internally in one of the 
# plotting functions)
def ss_two_predictors(bs_and_c, x1_vector, x2_vector, y_vector):
    """ Sum of squares error for intercept and a pair of slopes.
    """
    # unpack the list containing the slope and the intercept (this now has an extra slope!)
    b_1, b_2, c = bs_and_c 
    
    # calculate the fitted values, for this slope/intercept pairing (this now has an extra slope and extra vector!)
    fitted_vector = b_1*x1_vector + b_2*x2_vector + c 
    
    # calculate the error vector (this is the same process as for a single predictor)
    error = y_vector - fitted_vector
    
    # return the value of the cost function (this is the same process as for a single predictor)
    return np.sum(error ** 2)
```

```{python}
# read in the data
duncan = pd.read_csv("data/Duncan_Occupational_Prestige.csv")

# show the data
duncan
```

```{python}
prestige = duncan['prestige'].values

education = duncan['education'].values

income = duncan['income'].values
```

## Statistical adjustment, with the Duncan data, continuous predictors

```{python}
# do not worry about this code, it is just a convenience function to plot the data
def make_scatter(with_errors=False,
                 show=False,
                 return_errors=False,
                 b=1,
                 c=1,
                 x=np.array([]),
                 y= np.array([]),
                 xlabel='',
                 ylabel='',
                 legend_loc=(1, 0.6),
                 continuous_line=False,
                 model_string='',
                 round_to=2):
    plt.scatter(x, y, label='Actual values ($y$)')
    # plot the predicted values
    fitted = b * x + c
    if continuous_line == False:
        plt.plot(x, fitted, 'ro', label='Fitted values from linear regression ($\hat{y}$)')
    elif continuous_line == True:
        x_for_plot = np.linspace(x.min(), x.max())
        fitted_for_plot = b * x_for_plot + c
        plt.plot(x_for_plot, fitted_for_plot, '--', color = 'red', label='Edge of linear regression plane ($\hat{y}$)')
    if with_errors == True:
        # plot the distance between predicted and actual, for all points.
        n = len(x)
        for i in range(n):
            plt.plot([x[i], x[i]], [fitted[i], y[i]], 'k:')
        # the following code line is just to trick Matplotlib into making a new
        # a single legend entry for the dotted lines.
        plt.plot([], [], 'k:', label='Errors ($ \\varepsilon $)')
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(model_string+f"\n$b$ = {round(b,round_to)} \n$c$ = {round(c,round_to)} \n Sum of Squared Error = {round(np.sum((y - (b*x + c))**2), 2)}")
    # show the legend
    plt.legend(loc = legend_loc);
    if show == True:
        plt.show()
    # return and show the error vector?
    if return_errors == True:
        errors = y - fitted
        jupyprint(f"Here is the error vector for the current line: {arraytex(np.atleast_2d(errors.round(2)).T)}")
        jupyprint(f"The sum of the squared error is <b> {round(np.sum((errors)**2),2)}. </b>")
        return errors 
 
# some other convenience functions
def make_scatter_line_comparison(b, c):
    # Call the make_scatter function with some defaults.
    errors = make_scatter(with_errors = True,
                          return_errors = True,
                          b = b,
                          c = c,
                          show = True)
    return errors

# do not worry about this code, iit is just to generate the 3D plots
def make_3d_scatter(x1, x2, y,
                    x1_slope = 1,
                    x2_slope = 1,
                    c =  1, 
                   x1_label = '',
                   x2_label = '',
                   y_label = '',
                   return_errors = False,
                   show = True,
                   plane_alpha = 0.5,
                   model_string ='',
                   round_to=2):
    sum_sq = ss_two_predictors([x1_slope, x2_slope, c], x1, x2, y)
    ax = plt.figure(figsize=(8,8)).add_subplot(111, projection='3d')
    ax.scatter(x1,x2,y, label = 'Actual values ($y$)')
    ax.set_xlabel(x1_label)
    ax.set_ylabel(x2_label)
    ax.set_zlabel(y_label)
    mx_x1 = x1.max()
    mx_x2 = x2.max()
    mx_y = y.max()
    min_x1 = np.min([0, x1.min()])
    min_x2 = np.min([0, x2.min()])
    min_y = np.min([0, y.min()])
    # Plot the fitting plane.
    plane_x = np.linspace(0, mx_x1, 50)
    plane_y = np.linspace(0, mx_x2, 50)
    X, Y = np.meshgrid(plane_x, plane_y)
    Z = c + x1_slope * X + x2_slope * Y
    ax.plot_wireframe(X,Y,Z, color = 'red', label = 'Linear regression plane', alpha = plane_alpha)
    # Plot lines between each point and fitting plane
    for i in np.arange(len(y)):
            x1_point, x2_point, actual = x1[i], x2[i], y[i]
            fitted = c + x1_point * x1_slope + x2_point * x2_slope
            ax.plot([x1_point, x1_point], [x2_point, x2_point], [fitted, actual],
                    linestyle=':',
                    linewidth=0.5,
                    color='black')
    # add labels to error
    ax.plot([], [], [],
        linestyle=':',
        linewidth=0.5,
        color='black',
        label = 'Errors ($ \\varepsilon $)')
    # Set the axis limits (and reverse y axis)
    ax.set_xlim(min_x1, mx_x1)
    ax.set_ylim(min_x2, mx_x2)
    ax.set_zlim(min_y, mx_y)
    ax.zaxis.labelpad=-3
    # show the legend
    plt.legend()
    plt.title(model_string+f"\n$b_1$ = {round(x1_slope,round_to)} \n$b_2$ = {round(x2_slope,round_to)} \n$c$ = {round(c,round_to)} \n Sum of Squared Error = {round(sum_sq, 2)}")
    if show == True:
        plt.show()
    if return_errors == True:
        fitted = c + x1_slope * x1 + x2_slope*x2
        errors = y - fitted
        jupyprint(f"Here is the error vector for the current regression plane: {arraytex(np.atleast_2d(errors.round(2)).T)}")
        jupyprint(f"The sum of the squared error is <b> {round(np.sum((errors)**2), 2)} </b>.")
        return errors 

# some convenience plotting functions
def plot_model_3D(x1_slope, x2_slope, c, return_errors = True):
    errors = make_3d_scatter(education, income, prestige,
               x1_slope = x1_slope, 
               x2_slope = x2_slope,
               c = c,
               return_errors = return_errors,
               x1_label="Education",
               x2_label="Income",
               y_label="Prestige")
    return errors
```

### Prestige ~ Education

```{python}
duncan_prestige_education = smf.ols('prestige ~ education', data = duncan).fit()

duncan_prestige_education.summary(slim = True)
```

```{python}
duncan_prestige_education_slope = duncan_prestige_education.params['education']

duncan_prestige_education_slope
```

```{python}
duncan_prestige_education_intercept = duncan_prestige_education.params['Intercept']

duncan_prestige_education_intercept
```

```{python}
def plot_prestige_education(continuous_line=False):
    make_scatter(x=education,
    y=prestige,
    b=duncan_prestige_education_slope, 
    c=duncan_prestige_education_intercept, 
    xlabel='Education',
    ylabel='Prestige',
    with_errors = True,
    model_string = 'Prestige ~ Education',
    continuous_line=continuous_line)
    
plot_prestige_education()
```

```{python}
def fit_single_pred_model(model_spec_string,
                          dataset,
                          slope_name,
                          slim_summary = True):

    model = smf.ols(model_spec_string, data = dataset).fit()
    
    display(model.summary(slim = slim_summary))
    
    intercept = model.params['Intercept']
    
    slope = model.params[slope_name]
    
    return model, intercept, slope

test_model, test_intercept, test_slope = fit_single_pred_model('prestige ~ education',
                                                               duncan,
                                                               'education')
```

```{python}
assert test_intercept == duncan_prestige_education_intercept
```

```{python}
assert test_slope == duncan_prestige_education_slope
```

### Prestige ~ Income

```{python}
(duncan_prestige_income,
 duncan_prestige_income_intercept,
 duncan_prestige_income_slope) = fit_single_pred_model('prestige ~ income', duncan, slope_name = 'income')
```

```{python}
def plot_prestige_income(continuous_line=False):
    make_scatter(x=income,
    y=prestige,
    b=duncan_prestige_income_slope, 
    c=duncan_prestige_income_intercept, 
    xlabel='Income',
    ylabel='Prestige',
    with_errors = True,
    model_string = 'Prestige ~ Income',
    continuous_line=continuous_line)
    
plot_prestige_income()
```

### Prestige ~ Education + Income

```{python}
def interactive_plane_fitter(education_slope, income_slope, intercept):

    # calculate the fitted values, for this combination of parameter estimates
    fitted = education_slope * education + income_slope * income + intercept

    # calculate the errors, for this combination of parameter estimates
    errors = prestige - fitted

    # compare the best fitting plane to the one generated by our guessed parameters
    errors3d_from_guesses = plot_model_3D(x1_slope = education_slope, 
                                        x2_slope = income_slope,
                                        c = intercept,
                                        return_errors = False)

    # do not worry about this code, it just prints the mathematical notation below this cell
    jupyprint(f"${arraytex(np.atleast_2d(fitted + errors).T)} = {round(education_slope, 2)} * {arraytex(np.atleast_2d(education).T)} + {round(income_slope, 2)} * {arraytex(np.atleast_2d(income).T)} + {round(intercept, 2)} + {arraytex(np.atleast_2d(errors).round(2).T)}$")
    jupyprint(f"The sum of the squared errors for this combination of parameter estimates is <b> {np.sum(errors**2)} </b>")
    
interact(interactive_plane_fitter, education_slope = (-1, 1, 0.1), income_slope = (-1, 1, 0.1), intercept = (-10, 10, 0.1))
```

```{python}
duncan_prestige_education_income = smf.ols('prestige ~ education + income', data = duncan).fit()

duncan_prestige_education_income.summary(slim = True)
```

```{python}
def fit_model(model_spec_string,
                     dataset,
                     slope_names_list,
                     slim_summary = True):

    model = smf.ols(model_spec_string, data = dataset).fit()
    
    display(model.summary(slim = slim_summary))
    
    intercept = model.params['Intercept']
    
    if len(slope_names_list) == 1:

        slope = model.params[slope_names_list[0]]

        return model, intercept, slope
    
    if len(slope_names_list) == 2:
        
        slope_1 = model.params[slope_names_list[0]]
        
        slope_2 = model.params[slope_names_list[1]]
        
        return model, intercept, slope_1, slope_2
```

```{python}
test_model, test_intercept, test_slope_1, test_slope_2 = fit_model('prestige ~ education + income',
                                                         duncan,
                                                         ['education', 'income'])

assert test_intercept == duncan_prestige_education_income.params['Intercept']
assert test_slope_1 == duncan_prestige_education_income.params['education']
assert test_slope_2 == duncan_prestige_education_income.params['income']
```

```{python}
(duncan_prestige_education_income,
 duncan_prestige_education_income_intercept,
 duncan_prestige_education_income_ed_slope,
 duncan_prestige_education_income_inc_slope) = fit_model('prestige ~ education + income',
                                                         duncan,
                                                         ['education', 'income'])
```

```{python}
make_3d_scatter(education,
                income, 
                prestige,
                x1_slope=duncan_prestige_education_income_ed_slope,
                x2_slope=duncan_prestige_education_income_inc_slope,
                c=duncan_prestige_education_income_intercept,
                x1_label='Education',
                x2_label='Income',
                y_label = 'Prestige',
                model_string='Prestige ~ Education + Income')
```

```{python}
# do not worry about this code, it is just to generate the 3D plots
def single_multi_comparison_subplots(x1 = education, x2 = income, y = prestige,
                                     x1_label = 'Education',
                                       x2_label = 'Income',
                                       y_label = 'Prestige',
                                       plane_alpha = 0.5,
                                       x1_0_and_1 = False,
                                       x2_0_and_1 = False,
                                       round_to = 2,
                                        model_strings = ["`prestige ~ education` \n(ignoring `income`)",
                                                         "`prestige ~ income` \n(ignoring `education`)",
                                                         "`prestige ~ education + income`"],
                                    x1_slopes=[duncan_prestige_education_slope, 0, duncan_prestige_education_income_ed_slope],
                                    x2_slopes=[0, duncan_prestige_income_slope, duncan_prestige_education_income_inc_slope],
                                    cs = [duncan_prestige_education_intercept, duncan_prestige_income_intercept,
                                          duncan_prestige_education_income_intercept]):
    fig, axes = plt.subplots(1, 3, figsize=(16, 10), subplot_kw={'projection': '3d'})
   
    for num, (subplot_val, x1_slope, x2_slope, c) in enumerate(zip([131, 132, 133],
                                                               x1_slopes,
                                                               x2_slopes,
                                                               cs)):
        sum_sq = ss_two_predictors([x1_slope, x2_slope, c], x1, x2, y)
        ax = axes[num]
        ax.scatter(x1,x2,y, label = 'Actual values ($y$)')
        if x1_0_and_1 == True:
            ax.set_xticks([0, 1])
        if x2_0_and_1 == True:
            ax.set_yticks([0, 1])
        ax.set_xlabel(x1_label)
        ax.set_ylabel(x2_label)
        ax.set_zlabel(y_label)
        mx_x1 = x1.max()
        mx_x2 = x2.max()
        mx_y = y.max()
        min_x1 = np.min([0, x1.min()])
        min_x2 = np.min([0, x2.min()])
        min_y = np.min([0, y.min()])
        # Plot the fitting plane.
        plane_x = np.linspace(min_x1, mx_x1, 50)
        plane_y = np.linspace(min_x2, mx_x2, 50)
        X, Y = np.meshgrid(plane_x, plane_y)
        Z = c + x1_slope * X + x2_slope * Y
        ax.plot_wireframe(X,Y,Z, color = 'red', label = 'Linear regression plane', alpha = plane_alpha)
        # Plot lines between each point and fitting plane
        for i in np.arange(len(y)):
            x1_point, x2_point, actual = x1[i], x2[i], y[i]
            fitted = c + x1_point * x1_slope + x2_point * x2_slope
            ax.plot([x1_point, x1_point], [x2_point, x2_point], [fitted, actual],
                    linestyle=':',
                    linewidth=0.5,
                    color='black')
        # add labels to error
        ax.plot([], [], [],
            linestyle=':',
            linewidth=0.5,
            color='black',
            label = 'Errors ($ \\varepsilon $)')
        # Set the axis limits (and reverse y axis)
        ax.set_xlim(min_x1, mx_x1)
        ax.set_ylim(min_x2, mx_x2)
        ax.set_zlim(min_y, mx_y)
        ax.set_title(f"{model_strings[num]}\n$b_1$ = {round(x1_slope,round_to)} \n$b_2$ = {round(x2_slope,round_to)} \n$c$ = {round(c,round_to)} \n Sum of Squared Error = {round(sum_sq, 2)}")
    plt.legend(loc = (1,  1))
    
single_multi_comparison_subplots()
```

[Imagine this as a physical object]

![](images/stat_adjust_duncan_ed_inc.png)


![](images/stat_adjust_duncan_ed.png)

```{python}
plot_prestige_education(continuous_line=True)
```

![](images/stat_adjust_duncan_inc.png)

```{python}
plot_prestige_income(continuous_line=True)
```

```{python}
single_multi_comparison_subplots()
```

## Statistical adjustment, with the EPL data, continuous predictors


[EPL data, using 3D plots above]

```{python}
epl = pd.read_csv('data/premier_league_2021.csv')
epl.head()
```

```{python}
goal_difference = epl['goal_difference'].values

defense = epl['defense'].values

forward = epl['forward'].values
```

## Goal Difference ~ Forward

```{python}
(epl_goaldiff_forward,
epl_goaldiff_forward_intercept, 
epl_goaldiff_forward_slope) = fit_model(model_spec_string='goal_difference ~ forward', 
                                  dataset=epl,
                                  slope_names_list=['forward'])
```

```{python}
def plot_goaldiff_forward(continuous_line=False):
    make_scatter(x=forward,
    y=goal_difference,
    b=epl_goaldiff_forward_slope, 
    c=epl_goaldiff_forward_intercept, 
    xlabel='Forward Spending',
    ylabel='Goal Difference',
    with_errors = True,
    model_string = 'Goal Difference ~ Forward',
    continuous_line=continuous_line,
    round_to=4)
    
plot_goaldiff_forward()
```

## Goal Difference ~ Defense

```{python}
(epl_goaldiff_defense,
epl_goaldiff_defense_intercept, 
epl_goaldiff_defense_slope) = fit_model(model_spec_string='goal_difference ~ defense', 
                                  dataset=epl,
                                  slope_names_list=['defense'])
```

```{python}
def plot_goaldiff_defense(continuous_line=False):
    make_scatter(x=defense,
    y=goal_difference,
    b=epl_goaldiff_defense_slope, 
    c=epl_goaldiff_defense_intercept, 
    xlabel='Defense Spending',
    ylabel='Goal Difference',
    with_errors = True,
    model_string = 'Goal Difference ~ Defense',
    continuous_line=continuous_line,
    round_to=4)
    
plot_goaldiff_defense()
```

## Goal Difference ~ Forward + Defense

```{python}
(epl_goaldiff_forward_defense,
epl_goaldiff_forward_defense_intercept, 
epl_goaldiff_forward_defense_slope_forw,
epl_goaldiff_forward_defense_slope_def) = fit_model(model_spec_string='goal_difference ~ forward + defense', 
                                          dataset=epl,
                                          slope_names_list=['forward', 'defense'])
```

```{python}
make_3d_scatter(x1=forward,
                x2=defense, 
                y=goal_difference,
                x1_slope=epl_goaldiff_forward_defense_slope_forw,
                x2_slope=epl_goaldiff_forward_defense_slope_def,
                c=epl_goaldiff_forward_defense_intercept,
                x1_label='Forward Spending',
                x2_label='Defense Spending',
                y_label = 'Goal Difference',
                model_string='Goal Difference ~ Forward Spending + Defense Spending', 
                round_to=4)
```

```{python}
single_multi_comparison_subplots(x1=forward,
                                 x2=defense,
                                 y=goal_difference,
                                 x1_label='Forward Spending',
                                 x2_label='Defense Spending',
                                 y_label='Goal Difference',
                                 round_to=4,
                                 model_strings = ["`goal_difference ~ forward` \n(ignoring `defense`)",
                                                         "`goal_difference ~ defense` \n(ignoring `forward`)",
                                                         "`goal_difference ~ forward + defense`"],
                                    x1_slopes=[epl_goaldiff_forward_slope, 0, epl_goaldiff_forward_defense_slope_forw],
                                    x2_slopes=[0, epl_goaldiff_defense_slope, epl_goaldiff_forward_defense_slope_def],
                                    cs = [epl_goaldiff_forward_intercept, epl_goaldiff_defense_intercept,
                                          epl_goaldiff_forward_defense_intercept])
```

## Statistical adjustment, with the Duncan data, continuous and categorical predictors



```{python}
type_orig = duncan['type']

duncan = pd.get_dummies(duncan, columns = ['type'], drop_first = True)

duncan.head()
```

```{python}
duncan['type'] = type_orig

duncan.head(30)
```

```{python}
duncan[duncan['type'] == 'bc']
```

```{python}
wc_dummy = duncan['type_wc'].values

prof_dummy = duncan['type_prof'].values
```

## Statistical adjustment, with the Duncan data, only categorical predictors

```{python}

```
