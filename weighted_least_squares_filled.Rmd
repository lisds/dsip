---
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.16.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
  orphan: true
---

# Fitting models with different cost functions

```{python}
import numpy as np
np.set_printoptions(suppress=True)
from scipy.optimize import minimize
import pandas as pd
pd.set_option('mode.copy_on_write', True)
import statsmodels.formula.api as smf
import sklearn.linear_model as sklm
import sklearn.metrics as skmetrics
```

```{python}
df = pd.read_csv('data/rate_my_course.csv')
#MB To make it easier to run Statsmodels, in particular.
df = df.rename(columns={'Overall Quality': 'Quality'})
df
```

Fetch some columns of interest:

```{python}
# This will be our y (the variable we predict).
quality = df['Quality']
# One of both of these will be our X (the predictors).
clarity_easiness = df[['Clarity', 'Easiness']]
```

Fit the model with Statsmodels.

```{python}
sm_model = smf.ols('Quality ~ Clarity + Easiness', data=df)
sm_fit = sm_model.fit()
sm_fit.summary()
```

Fit the same model with Scikit-learn.

```{python}
sk_fit = sklm.LinearRegression().fit(clarity_easiness, quality)
sk_fit.coef_
```

```{python}
sk_fit.intercept_
```

Compare the parameters to Statsmodels.

```{python}
sm_fit.params
```

## The fitted values and Scikit-learn

The values predicted by the (Sklearn) model:

```{python}
fitted = sk_fit.predict(clarity_easiness)
fitted
```

Compare to Statsmodels:

```{python}
assert np.allclose(sm_fit.predict(), fitted)
```

Write a function to compute the fitted values given the parameters:


```{python}
def calc_fitted(params, X):
    X = np.array(X)
    n, p = X.shape
    fitted = np.zeros(n)
    # Add contribution of each regressor.
    for col_no in range(p):
        fitted += params[col_no] * X[:, col_no]
    # Add contribution of intercept (last parameter)
    return fitted + params[-1]
```

Compile Sklearn parameters, and use these to calculated fitted values with our function.  Compare, to show they are (near as dammit) similar.

```{python}
params = list(sk_fit.coef_) + [sk_fit.intercept_]
params
```

```{python}
assert np.allclose(calc_fitted(params, clarity_easiness), fitted)
```

## The long and the short of R^2^

Sklearn has an `r2_score` metric.

```{python}
skmetrics.r2_score(quality, fitted)
```

We already know the formula for R^2^.  We can calculate by hand to show this gives the same answer.

```{python}
ss_around_mean = ((quality - quality.mean()) ** 2).sum()
ss_around_mean
```

```{python}
resid = quality - fitted
ss_residuals = (resid ** 2).sum()
ss_residuals
```

```{python}
# Calculate R2
1 - ss_residuals / ss_around_mean
```

### R^2^ for another, reduced model

Fit a reduced model that is just "Clarity" without "Easiness".

```{python}
clarity_df = df[['Clarity']]
reduced_fit = sklm.LinearRegression().fit(clarity_df, quality)
reduced_fit.coef_, reduced_fit.intercept_
```

Calculate R^2^ for reduced model as compared to our full model.

```{python}
reduced_resid = quality - reduced_fit.predict(clarity_df)
ss_reduced_residuals = (reduced_resid ** 2).sum()
ss_reduced_residuals
```

```{python}
1 - ss_residuals / ss_reduced_residuals
```

## On weights, and a weighted mean

This is the usual mean:

```{python}
quality.mean()
```

Of course this is the same as:

```{python}
n = len(quality)
quality.sum() / n
```

or

```{python}
(quality * 1 / n).sum()
```

Calculate weights to weight values by number of professors, on the basis that larger number of professors may give more reliable values.

```{python}
n_professors = df['Number of Professors']
prop_professors = n_professors / np.sum(n_professors)
prop_professors
```

Calculate weighted mean.

```{python}
weights = prop_professors
(quality * weights).sum()
```

Numpy's version of same:

```{python}
np.average(quality, weights=weights)
```

## Fitting the model with minimize

A function for Sum of Squares, to use with `minimize`.

```{python}
def sos(params, X, y):
    residuals = y - calc_fitted(params, X)
    return np.sum(residuals ** 2)
```

Use `sos` function, in example call, and then with `minimize`.

```{python}
model_cols = df[['Clarity', 'Easiness']]
sos([0, 0, 0], model_cols, quality)
```

```{python}
res = minimize(sos, [0, 0, 0], args=(model_cols, quality))
res
```

```{python}
res.x
```

Compare to parameters with (e.g.) Sklearn.

```{python}
sk_fit.coef_, sk_fit.intercept_
```

## Fitting with weights

Statsmodels, weighted regression.

```{python}
wls_model = smf.wls('Quality ~ Clarity + Easiness', weights=weights, data=df)
wls_fit = wls_model.fit()
wls_fit.summary()
```

[Sklearn, weighted
regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).
Also see [Wikipedia on weighted
regression](https://en.wikipedia.org/wiki/Weighted_least_squares).

```{python}
sk_weights = sklm.LinearRegression().fit(clarity_easiness, quality,
sample_weight=weights)
sk_weights.coef_, sk_weights.intercept_
```

The `minimize` cost function for weighted regression:

```{python}
def sos_weighted(params, X, y, weights):
    residuals = y - calc_fitted(params, X)
    return np.sum((residuals ** 2) * weights)
```

```{python}
wls_res = minimize(sos_weighted, [0, 0, 0], args=(model_cols, quality,
weights))
wls_res
```

```{python}
wls_res.x
```

```{python}
wls_fit.params
```

## Penalized regression

Penalized regression is where you simultaneously minimize some cost related to the model (mis-)fit, and some cost related to the parameters of your model.

### Ridge regression

For example, in [ridge
regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html),
with try and minimize the sum of squared residuals _and_ the sum of squares
of the parameters (apart from the intercept).

```{python}
sk_ridge = sklm.Ridge(alpha=1.0).fit(clarity_easiness, quality)

sk_ridge.coef_, sk_ridge.intercept_
```

Fit with the `minimize` cost function:

```{python}
def sos_ridge(params, X, y, alpha):
    residuals = y - calc_fitted(params, X)
    ss_resid = np.sum(residuals ** 2)
    penalty = np.sum(params[:-1] ** 2)
    return ss_resid + alpha * penalty
```

```{python}
alpha = 1.0
res_ridge = minimize(sos_ridge, [0, 0, 0], args=(clarity_easiness, quality, alpha))
res_ridge.x
```

### LASSO


See the [Scikit-learn LASSO page](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html).


As noted there, the cost function is:

$$
\frac{1}{2 * \text{n_samples}} * ||y - Xw||^2_2 + alpha * ||w||_1
$$

$w$ refers to the vector of model parameters.

This part of the equation:

$$
||y - Xw||^2_2
$$

is the sum of squares of the residuals, because the residuals are $y - Xw$
(where $w$ are the parameters of the model, and $Xw$ are therefore the fitted
values), and the $||y - Xw||^2_2$ refers to the squared [L2 vector
norm](https://mathworld.wolfram.com/L2-Norm.html), which is the same as the
sum of squares.

$$
||w||_1
$$

is the [L1 vector norm](https://mathworld.wolfram.com/L1-Norm.html) of the
parameters $w$, which is the sum of the absolute values of the parameters.

Let's do that calculation, with a low `alpha` (otherwise both slopes get
forced down to zero):

```{python}
# We need LassoLars for increased accuracy.
sk_lasso = sklm.LassoLars(alpha=0.01).fit(clarity_easiness, quality)

sk_lasso.coef_, sk_lasso.intercept_
```

```{python}
def sos_lasso(params, X, y, alpha):
    n_samples = len(y)
    residuals = y - calc_fitted(params, X)
    ss_resid = np.sum(residuals ** 2)
    penalty = np.sum(np.abs(params[:-1]))
    return 1 / (2 * n_samples) * ss_resid + alpha * penalty
```

```{python}
# Change optimization method and tighten tolerance to get very close to
# LassoLars answer.
res_lasso = minimize(sos_lasso, [0, 0, 0], args=(clarity_easiness, quality, 0.01),
                     method='powell', tol=1e-10)
res_lasso
```

```{python}
res_lasso.x
```

## Cross-validation

Should I add the "Easiness" regressor?

```{python}
def drop_and_predict(df, x_cols, y_col, to_drop):
    out_row = df.loc[to_drop:to_drop]  # Row to drop, as a data frame
    out_df = df.drop(index=to_drop)  # Dataframe without dropped row.
    # Fit on everything but the dropped row.
    fit = sklm.LinearRegression().fit(out_df[x_cols], out_df[y_col])
    # Use fit to predict the dropped row.
    fitted = fit.predict(out_row[x_cols])
    return fitted[0]
```

Fit the larger model, with "Easiness", and drop / predict each "Quality" value.

```{python}
out_quality = np.zeros(n)
for i in range(n):
    out_quality[i] = drop_and_predict(df, ['Clarity', 'Easiness'], 'Quality', i)
out_quality
```

Calculate the sum of squared error:

```{python}
np.sum((quality - out_quality) ** 2)
```

Fit the smaller model, omitting "Easiness", and drop / predict each "Quality"
value.

```{python}
out_quality_reduced = np.zeros(n)
for i in range(n):
    out_quality_reduced[i] = drop_and_predict(df, ['Clarity'], 'Quality', i)
out_quality_reduced
```

How is the sum of squared error for this reduced model?

```{python}
np.sum((quality - out_quality_reduced) ** 2)
```
